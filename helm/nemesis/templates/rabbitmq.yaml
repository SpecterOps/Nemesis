---
apiVersion: v1
kind: ServiceAccount
metadata:
  labels:
    app.kubernetes.io/name: nemesis
    app.kubernetes.io/component: rabbitmq-server
  name: nemesis-rabbitmq-sa
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  labels:
    app.kubernetes.io/name: nemesis
    app.kubernetes.io/component: rabbitmq-server
  name: nemesis-rabbitmq-endpoint-reader
rules:
- apiGroups:
  - ""
  resources:
  - endpoints
  verbs:
  - get
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  labels:
    app.kubernetes.io/name: nemesis
    app.kubernetes.io/component: rabbitmq-server
  name: nemesis-rabbitmq-endpoint-reader
roleRef:
  kind: Role
  name: nemesis-rabbitmq-endpoint-reader
  apiGroup: rbac.authorization.k8s.io
subjects:
- kind: ServiceAccount
  name: nemesis-rabbitmq-sa
---
apiVersion: v1
kind: ConfigMap
metadata:
  labels:
    app.kubernetes.io/component: rabbitmq-server
    app.kubernetes.io/name: nemesis-rabbitmq
  name: nemesis-rabbitmq-config
data:
  enabled_plugins: |
    [rabbitmq_management,rabbitmq_peer_discovery_k8s,rabbitmq_prometheus].

  rabbitmq.conf: |
    ## Clustering
    cluster_formation.peer_discovery_backend = rabbit_peer_discovery_k8s
    cluster_formation.k8s.host = kubernetes.default.svc.cluster.local
    # Use the Pods hostname instead of IP addresses to create a cluster.
    # The Pod IP doesn't work with persistence because new Pods get a new IP addresses,
    # that doesn't have access to existing data after the node name has changed.
    cluster_formation.k8s.address_type = hostname
    cluster_formation.node_cleanup.interval = 10
    # Set to false if automatic removal of unknown/absent nodes
    # is desired. This can be dangerous, see
    #  * http://www.rabbitmq.com/cluster-formation.html#node-health-checks-and-cleanup
    #  * https://groups.google.com/forum/#!msg/rabbitmq-users/wuOfzEywHXo/k8z_HWIkBgAJ
    cluster_formation.node_cleanup.only_log_warning = true
    cluster_partition_handling = autoheal
    ## queue master locator
    queue_master_locator = min-masters
    ## The default "guest" user is only permitted to access the server
    ## via a loopback interface (e.g. localhost)
    ## See http://www.rabbitmq.com/access-control.html#loopback-users
    loopback_users.guest = false

    management.path_prefix = /rabbitmq/
---
{{ if eq .Values.operation.environment "production" }}
apiVersion: v1
kind: PersistentVolume
metadata:
  name: rabbitmq-data-pv
  labels:
    type: local
    app: rabbitmq
spec:
  storageClassName: manual
  capacity:
    storage: {{ .Values.rabbitmq.storage }}
  accessModes:
    - ReadWriteOnce
  persistentVolumeReclaimPolicy: Retain
  hostPath:
    path: "/mnt/data/rabbitmq/"
{{ end }}
---
{{ if eq .Values.operation.environment "production" }}
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: nemesis-rabbitmq-pvc
  labels:
    app: rabbitmq
spec:
  storageClassName: manual
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: {{ .Values.rabbitmq.storage }}
  volumeName: rabbitmq-data-pv
{{ end }}
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  labels:
    app.kubernetes.io/name: nemesis
    app.kubernetes.io/component: rabbitmq-server
  name: nemesis-rabbitmq
spec:
  podManagementPolicy: OrderedReady
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: nemesis
      app.kubernetes.io/component: rabbitmq-server
  serviceName: nemesis-rabbitmq-discovery
  updateStrategy:
    type: RollingUpdate
  template:
    metadata:
      labels:
        app.kubernetes.io/name: nemesis
        app.kubernetes.io/component: rabbitmq-server
    spec:
      containers:
        - env:
          - name: JAVA_OPTS
            value: -Dlog4j2.formatMsgNoLookups=true -Dlog4j2.disable.jmx=true
          - name: MY_POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: RABBITMQ_USE_LONGNAME
            value: "true"
          - name: RABBITMQ_NODENAME
            value: rabbit@$(MY_POD_NAME).nemesis-rabbitmq-discovery.default.svc.cluster.local
          - name: K8S_SERVICE_NAME
            value: nemesis-rabbitmq-discovery
          - name: K8S_HOSTNAME_SUFFIX
            value: .nemesis-rabbitmq-discovery.default.svc.cluster.local
          - name: RABBITMQ_DEFAULT_USER
            valueFrom:
              secretKeyRef:
                key: rabbitmq-admin-user
                name: {{ .Values.rabbitmq.existingSecret }}
          - name: RABBITMQ_DEFAULT_PASS
            valueFrom:
              secretKeyRef:
                key: rabbitmq-admin-password
                name: {{ .Values.rabbitmq.existingSecret }}
          image: rabbitmq:3.11.7
          imagePullPolicy: IfNotPresent
          lifecycle:
            postStart:
              exec:
                command:
                - /bin/bash
                - -c
                - |
                  # Wait for the RabbitMQ to be ready.
                  until rabbitmqctl node_health_check; do
                    sleep 5
                  done

                  # By default, RabbitMQ does not have Highly Available policies enabled,
                  # using the following command to enable it.
                  rabbitmqctl set_policy ha-all "." '{"ha-mode":"all", "ha-sync-mode":"automatic"}' --apply-to all --priority 0
          name: rabbitmq
          ports:
            - containerPort: 25672
              name: clustering
              protocol: TCP
            - containerPort: 5672
              name: amqp
              protocol: TCP
            - containerPort: 5671
              name: amqp-ssl
              protocol: TCP
            - containerPort: 15692
              name: prometheus
              protocol: TCP
            - containerPort: 15672
              name: http
              protocol: TCP
          resources: {{ toYaml .Values.rabbitmq.resources | nindent 12}}
          volumeMounts:
            - mountPath: /etc/rabbitmq
              name: config
            - mountPath: /var/lib/rabbitmq
              name: nemesis-rabbitmq-data
          livenessProbe:
            exec:
              command:
                - rabbitmqctl
                - status
            initialDelaySeconds: 60
            timeoutSeconds: 30
            periodSeconds: 60
          readinessProbe:
            exec:
              command:
                - rabbitmqctl
                - status
            initialDelaySeconds: 5
            timeoutSeconds: 10
            periodSeconds: 5
      initContainers:
        - command:
          - /bin/bash
          - -euc
          - |
            # Remove cached erlang cookie since we are always providing it,
            # that opens the way to recreate the application and access to existing data
            # as a new erlang will be regenerated again.
            echo ${RABBITMQ_ERLANG_COOKIE} > /var/lib/rabbitmq/.erlang.cookie
            chmod 600 /var/lib/rabbitmq/.erlang.cookie

            # Copy the mounted configuration to both places.
            cp /rabbitmqconfig/rabbitmq.conf /etc/rabbitmq/rabbitmq.conf
            # Change permission to allow to add more configurations via variables
            chown :999 /etc/rabbitmq/rabbitmq.conf
            chmod 660 /etc/rabbitmq/rabbitmq.conf
            cp /rabbitmqconfig/enabled_plugins /etc/rabbitmq/enabled_plugins
          env:
          - name: RABBITMQ_ERLANG_COOKIE
            valueFrom:
              secretKeyRef:
                name: {{ .Values.rabbitmq.existingSecret }}
                key: rabbitmq-erlang-cookie
          image: debian:11
          imagePullPolicy: IfNotPresent
          name: copy-rabbitmq-config
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /rabbitmqconfig
            name: configmap
          - mountPath: /etc/rabbitmq
            name: config
          - mountPath: /var/lib/rabbitmq
            name: nemesis-rabbitmq-data
      restartPolicy: Always
      schedulerName: default-scheduler
      securityContext: {}
      serviceAccountName: nemesis-rabbitmq-sa
      terminationGracePeriodSeconds: 180
      volumes:
        - name: configmap
          configMap:
            defaultMode: 420
            items:
              - key: rabbitmq.conf
                path: rabbitmq.conf
              - key: enabled_plugins
                path: enabled_plugins
            name: nemesis-rabbitmq-config
        - name: config
          emptyDir: {}

        # Data persistence option #1 - No persistence
        {{ if eq .Values.operation.environment "production" }}
        # Data persistence option #2 - Node volume. Remove the emptydir volume and uncomment everything below (including the PersistentVolume/PersistentVolumeClaim)
        - name: nemesis-rabbitmq-data
          persistentVolumeClaim:
            claimName: nemesis-rabbitmq-pvc
        {{ else }}
        - name: nemesis-rabbitmq-data # to have the data wiped on each reboot (for dev)
          emptyDir: {}
        {{ end}}
---
# This headless service allows communication between RabbitMQ nodes via hostname instead of IP addresses.
# The clusterIP is set to None.
# See: https://kubernetes.io/docs/concepts/services-networking/service/#headless-services
apiVersion: v1
kind: Service
metadata:
  name: nemesis-rabbitmq-discovery
  labels:
    app.kubernetes.io/name: nemesis
    app.kubernetes.io/component: rabbitmq-server
spec:
  ports:
  - name: amqp
    port: 5672
  - name: amqp-ssl
    port: 5671
  - name: clustering
    port: 25672
  - name: prometheus
    port: 15692
  - name: http
    port: 15672
  selector:
    app.kubernetes.io/name: nemesis
    app.kubernetes.io/component: rabbitmq-server
  type: ClusterIP
  clusterIP: None
---
# This services is used to connect to the RabbitMQ using Port Forwarding
# or expose an external IP and run RabbitMQ cluster behind a LoadBalancer.
apiVersion: v1
kind: Service
metadata:
  name: nemesis-rabbitmq-svc
  labels:
    app.kubernetes.io/name: nemesis
    app.kubernetes.io/component: rabbitmq-server
spec:
  type: {{ .Values.rabbitmq.service.type }}
  selector:
    app.kubernetes.io/name: nemesis
    app.kubernetes.io/component: rabbitmq-server
  ports:
  - name: amqp
    port: {{ .Values.rabbitmq.service.amqp.port }}
  - name: amqp-ssl
    port: {{ .Values.rabbitmq.service.amqpSsl.port }}
  - name: clustering
    port: {{ .Values.rabbitmq.service.clustering.port }}
  - name: prometheus
    port: {{ .Values.rabbitmq.service.prometheus.port}}
  - name: http
    port: {{ .Values.rabbitmq.service.http.port }}
---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: rabbitmqadmin-ingress
  annotations:
    {{- if .Values.rabbitmq.ingress.annotations }}
    {{- .Values.rabbitmq.ingress.annotations | toYaml | nindent 4 }}
    {{- end }}
spec:
  ingressClassName: traefik
  rules:
    - http:
        paths:
          - path: /rabbitmq/
            pathType: Prefix
            backend:
              service:
                name: nemesis-rabbitmq-svc
                port:
                  number: 15672